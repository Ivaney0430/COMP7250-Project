# COMP7250-Project
This repository contains a comprehensive study on various Stochastic Gradient Descent (SGD) optimizers and their performance in different deep learning tasks. The project aims to evaluate and compare the convergence behavior and accuracy of multiple SGD variants, including: Standard SGD, Adam, RMSprop, and NAdam
